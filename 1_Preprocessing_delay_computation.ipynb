{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the spark application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.app.name': 'miaou_final'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>8919</td><td>application_1589299642358_3451</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3451/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3451_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8973</td><td>application_1589299642358_3509</td><td>pyspark</td><td>dead</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/cluster/app/application_1589299642358_3509\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster054.iccluster.epfl.ch:8188/applicationhistory/logs/iccluster068.iccluster.epfl.ch:45454/container_e06_1589299642358_3509_01_000001/container_e06_1589299642358_3509_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8983</td><td>application_1589299642358_3520</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3520/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3520_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>8996</td><td>application_1589299642358_3534</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3534/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3534_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9003</td><td>application_1589299642358_3541</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3541/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3541_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9028</td><td>application_1589299642358_3573</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3573/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3573_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9035</td><td>application_1589299642358_3582</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3582/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3582_01_000002/ebouille\">Link</a></td><td></td></tr><tr><td>9045</td><td>application_1589299642358_3590</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3590/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3590_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9054</td><td>application_1589299642358_3599</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3599/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3599_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9055</td><td>application_1589299642358_3601</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3601/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3601_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9058</td><td>application_1589299642358_3604</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3604/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3604_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9059</td><td>application_1589299642358_3605</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3605/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3605_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9062</td><td>application_1589299642358_3608</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3608/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3608_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9063</td><td>application_1589299642358_3609</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3609/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3609_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9065</td><td>application_1589299642358_3611</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3611/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster070.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3611_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9066</td><td>application_1589299642358_3613</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3613/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3613_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9067</td><td>application_1589299642358_3614</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3614/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3614_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9068</td><td>application_1589299642358_3615</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3615/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3615_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9070</td><td>application_1589299642358_3618</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3618/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3618_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9071</td><td>application_1589299642358_3620</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3620/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3620_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9072</td><td>application_1589299642358_3621</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3621/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3621_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9073</td><td>application_1589299642358_3622</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3622/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3622_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9074</td><td>application_1589299642358_3623</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3623/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3623_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9075</td><td>application_1589299642358_3624</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3624/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster066.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3624_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9076</td><td>application_1589299642358_3630</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3630/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster071.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3630_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9077</td><td>application_1589299642358_3634</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3634/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3634_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9078</td><td>application_1589299642358_3637</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3637/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3637_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9079</td><td>application_1589299642358_3638</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3638/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster068.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3638_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9080</td><td>application_1589299642358_3639</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3639/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster072.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3639_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9081</td><td>application_1589299642358_3641</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3641/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3641_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9082</td><td>application_1589299642358_3642</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3642/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster069.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3642_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9083</td><td>application_1589299642358_3643</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3643/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3643_01_000001/ebouille\">Link</a></td><td></td></tr><tr><td>9084</td><td>application_1589299642358_3644</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3644/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster067.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3644_01_000001/ebouille\">Link</a></td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure\n",
    "{\"conf\": {\n",
    "    \"spark.app.name\": \"miaou_final\"\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>9085</td><td>application_1589299642358_3645</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster044.iccluster.epfl.ch:8088/proxy/application_1589299642358_3645/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster065.iccluster.epfl.ch:8042/node/containerlogs/container_e06_1589299642358_3645_01_000001/ebouille\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, col, collect_list\n",
    "import numpy as np\n",
    "import geopy.distance\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import explode, col, lit\n",
    "from pyspark.sql import Window\n",
    "from datetime import datetime\n",
    "from math import ceil\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf, array\n",
    "from pyspark.sql.functions import to_timestamp, date_format\n",
    "from math import ceil\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import DoubleType, ArrayType, FloatType\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import pandas_udf, col, collect_list\n",
    "from pyspark.sql.types import IntegerType\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all timetable and calculate distance of stops from Zurich station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stops = spark.read.csv('/data/sbb/timetables/csv/stops/2019/05/14/stops.txt', header=True)\n",
    "stop_times = spark.read.csv('/data/sbb/timetables/csv/stop_times/2019/05/14/stop_times.txt', header=True)\n",
    "trips=spark.read.csv('/data/sbb/timetables/csv/trips/2019/05/14/trips.txt', header=True)\n",
    "calendar=spark.read.csv('/data/sbb/timetables/csv/calendar/2019/05/14/calendar.txt', header=True)\n",
    "routes=spark.read.csv('/data/sbb/timetables/csv/routes/2019/05/14/routes.txt', header=True)\n",
    "\n",
    "@f.udf(\"float\")\n",
    "def vincenty_udf(x, y):\n",
    "    return geopy.distance.vincenty(x, y).km\n",
    "\n",
    "coords_zurich = f.struct(f.lit(47.378177), f.lit(8.540192))\n",
    "stops=stops.withColumn('Dist_to_zurich',vincenty_udf(coords_zurich, f.struct(stops.stop_lat.cast(\"double\"),stops.stop_lon.cast(\"double\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Walking table that links stations which are less than 500m away from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stops_table = stops.filter(stops.Dist_to_zurich<15)\n",
    "stops_table = stops_table.withColumn('stop_id_s',f.substring(stops_table.stop_id,0,7))\n",
    "stops_table = stops_table.dropDuplicates([\"stop_id_s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_stop = stops_table.toPandas()\n",
    "df_stop[\"Dist_to_station\"]=np.zeros(len(df_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def close_stations(lat, long, df):\n",
    "    \n",
    "    coords = (lat, long)\n",
    "    \n",
    "    df[\"Dist_to_station\"] = df.apply(lambda x : geopy.distance.vincenty(coords, (x.stop_lat, x.stop_lon)).km, axis=1)\n",
    "    return df[df[\"Dist_to_station\"]<0.5][\"stop_id_s\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_stop[\"close_stations\"] = df_stop.apply(lambda x : close_stations(x.stop_lat, x.stop_lon, df_stop), axis=1)\n",
    "df_stop = df_stop[[\"stop_id_s\", \"stop_lat\", \"stop_lon\", \"close_stations\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "walking_table = spark.createDataFrame(df_stop).withColumn(\"close_stations\", explode(\"close_stations\"))\n",
    "stops_table = stops_table.select(col(\"stop_id_s\").alias(\"close_stop_id_s\"), col(\"stop_lat\").alias(\"close_stop_lat\"), col(\"stop_lon\").alias(\"close_stop_lon\"))\n",
    "walking_table = walking_table.join(stops_table, walking_table.close_stations==stops_table.close_stop_id_s)\n",
    "walking_table = walking_table.withColumn(\"time\", vincenty_udf(f.struct(walking_table.stop_lat.cast(\"double\"), walking_table.stop_lon.cast(\"double\")), f.struct(walking_table.close_stop_lat.cast(\"double\"), walking_table.close_stop_lon.cast(\"double\"))))\n",
    "walking_table = walking_table.filter(walking_table.time>0)\n",
    "walking_table = walking_table.withColumn(\"time\", (20*walking_table.time).cast(\"int\")).select(\"stop_id_s\", \"close_stop_id_s\", \"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "walking_table.write.parquet(\"/user/eckes/walking_table.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join all timetables and add next stop to calculate travel time between two stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#join all timetables\n",
    "df=stop_times.join(stops,\"stop_id\")\\\n",
    ".join(trips,\"trip_id\")\\\n",
    ".join(calendar,\"service_id\")\\\n",
    ".join(routes,\"route_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#filter stations less than 15km from Zurich station and for trains that operates every weekday. Also ignore trains outside rush hours\n",
    "df=df.filter((df.Dist_to_zurich<=15)&(df.monday=='1')&(df.tuesday=='1')&(df.wednesday=='1')&(df.thursday=='1')&(df.friday=='1'))\\\n",
    ".filter((f.hour('departure_time')>6)&(f.hour('arrival_time')<19))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#New stop_id=parent station if available else it's the stop id\n",
    "df=df.withColumn('stop_id_s',f.substring(df.stop_id,0,7)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#get next station info\n",
    "df=df.withColumn(\"next_stop_id_s\", f.lead(df.stop_id_s).over(Window.partitionBy(df.trip_id).orderBy(df.arrival_time,df.direction_id,df.stop_sequence.cast(\"int\"))))\\\n",
    ".withColumn(\"next_arrival_time\", f.lead(df.arrival_time).over(Window.partitionBy(df.trip_id).orderBy(df.arrival_time,df.direction_id,df.stop_sequence.cast(\"int\"))))\\\n",
    ".withColumn(\"next_departure_time\", f.lead(df.departure_time).over(Window.partitionBy(df.trip_id).orderBy(df.arrival_time,df.direction_id,df.stop_sequence.cast(\"int\"))))\\\n",
    ".withColumn(\"next_stop_lat\", f.lead(df.stop_lat).over(Window.partitionBy(df.trip_id).orderBy(df.arrival_time,df.direction_id,df.stop_sequence.cast(\"int\"))))\\\n",
    ".withColumn(\"next_stop_lon\", f.lead(df.stop_lon).over(Window.partitionBy(df.trip_id).orderBy(df.arrival_time,df.direction_id,df.stop_sequence.cast(\"int\"))))\\\n",
    ".withColumn(\"next_stop_name\", f.lead(df.stop_name).over(Window.partitionBy(df.trip_id).orderBy(df.arrival_time,df.direction_id,df.stop_sequence.cast(\"int\"))))\\\n",
    ".withColumn(\"next_trip_id\", f.lead(df.trip_id).over(Window.partitionBy(df.trip_id).orderBy(df.arrival_time,df.direction_id,df.stop_sequence.cast(\"int\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#calculate travel time\n",
    "df=df.withColumn('Travel_time',(df.next_arrival_time.cast('timestamp').cast('long')-df.arrival_time.cast('timestamp').cast('long'))/60).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reading the SBB data in orc format\n",
    "sbb = spark.read.orc('/data/sbb/orc/istdaten') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the SBB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The SBB data has a few columns that we do not need, such as `betreiber_id, betreiber_abk, betreiber_name, linien_text, verkehrsmittel_text`, so we remove them.\n",
    "\n",
    "- We assume that the trips that are not part of the regular schedule (`zusatzfahrt_tf = True`) or those that were failed, cancelled, or not completed (`faellt_aus_tf = False`), as well as those that do not stop (`durchfahrt_tf = False`) should not be taken into consideration as they are corner cases and cannot be used to predict usual delays.\n",
    "\n",
    "- The `an/ab_prognose_status` indicates whether the data was real (`REAL`), forecast (`PROGNOSE`), estimated (`GESCHAETZT`),  or unknown (`UNBEKANNT`).  For the `an_prognose_status` for example, the following table shows the distribution of the sbb data:\n",
    "\n",
    "\t|`GESCHAETZT`| 90076185 |\n",
    "    \n",
    "\t|`PROGNOSE`| 603610258 |\n",
    "    \n",
    "\t|  `UNBEKANNT` | 12872226 |\n",
    "    \n",
    "\t|  `REAL`| 128140212|\n",
    "\n",
    "\tThe `REAL` data is obviously to be trusted, however, it constitues only 8% of the data. Consequently, we decide that the `GESCHAETZT` should also be taken into account in our predictions. \n",
    "    \n",
    "- However, we noticed that Trams only have a `PROGNOSE` status. Thus, in order to keep the tram-related data, we decide to keep the rows where the transport type is 'Tram' and the `an/ab_prognose_status` is `PROGNOSE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_to_drop = ['betreiber_id', 'betreiber_abk', 'betreiber_name', 'linien_text', 'verkehrsmittel_text']\n",
    "sbb = sbb.drop(*columns_to_drop)\n",
    "\n",
    "# We remove all trips where ZUSATZFAHRT_TF is true (not part of the regular schedule)\n",
    "sbb = sbb.filter(sbb.zusatzfahrt_tf == 'false')\n",
    "sbb = sbb.drop('zusatzfahrt_tf')\n",
    "\n",
    "# We remove all trips where FAELLT_AUS_TF is true (trip failed, cancelled or not completed)\n",
    "sbb = sbb.filter(sbb.faellt_aus_tf == 'false')\n",
    "sbb = sbb.drop('faellt_aus_tf')\n",
    "\n",
    "# We only keep trips when AN_PROGNOSE_STATUS and AB_PROGNOSE_STATUS are REAL or GESCHAETZT (forecast) or PROGNOSE (estimated)\n",
    "sbb = sbb.filter((sbb.an_prognose_status == 'REAL') | (sbb.an_prognose_status == 'GESCHAETZT') |\\\n",
    "                 ((sbb.produkt_id == 'Tram') & (sbb.an_prognose_status == 'PROGNOSE')))\n",
    "# sbb = sbb.filter((sbb.an_prognose_status == 'REAL') | (sbb.an_prognose_status == 'GESCHAETZT'))\n",
    "sbb = sbb.drop('an_prognose_status')\n",
    "sbb = sbb.filter((sbb.ab_prognose_status == 'REAL') | (sbb.ab_prognose_status == 'GESCHAETZT') |\\\n",
    "                 ((sbb.produkt_id == 'Tram') & (sbb.ab_prognose_status == 'PROGNOSE')))\n",
    "# sbb = sbb.filter((sbb.ab_prognose_status == 'REAL') | (sbb.ab_prognose_status == 'GESCHAETZT'))\n",
    "sbb = sbb.drop('ab_prognose_status')\n",
    "\n",
    "# When the means of transport drives straight through, drop the rows (durchfahrt_tf = true)\n",
    "sbb = sbb.filter(sbb.durchfahrt_tf == 'false')\n",
    "sbb = sbb.drop('durchfahrt_tf')\n",
    "\n",
    "# When the stop is the start or end of a journey, the corresponding columns will be empty (ANKUNFTSZEIT/ABFAHRTSZEIT).\n",
    "# We remove rows where the actual time data is empty\n",
    "sbb = sbb.filter((sbb.an_prognose != '') & (sbb.ab_prognose != '')) \n",
    "# We remove rows where ankunftszeit is empty (the stop is the start of a journey so we assume there is no delay for the departure)\n",
    "sbb = sbb.filter((sbb.ankunftszeit != ''))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Translating the column names to english\n",
    "sbb = sbb.select(col(\"betriebstag\").alias(\"date\"), col(\"fahrt_bezeichner\").alias(\"trip_id\"),\\\n",
    "                 col(\"linien_id\").alias(\"line_id\"), col(\"produkt_id\").alias(\"transport_type\"), col(\"umlauf_id\").alias(\"circuit_id\")\\\n",
    "                , col(\"haltestellen_name\").alias(\"stop_name\"), col(\"ankunftszeit\").alias(\"arrival_time\"), col(\"an_prognose\").alias(\"actual_arrival_time\")\\\n",
    "                , col(\"abfahrtszeit\").alias(\"departure_time\"), col(\"ab_prognose\").alias(\"actual_departure_time\"),col(\"bpuic\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep stations in a radius of 15km of Zürich\n",
    "# We simply join sbb with df since df has been filtered and only contains stations in a radius of 15km of Zürich\n",
    "\n",
    "# In particular, we remove any platform numbers from the stop_id if df \n",
    "gate_function = udf(lambda x: x.split(':')[0] if x != None else '', StringType())\n",
    "df = df.withColumn('stop_id_clean', gate_function('stop_id'))\n",
    "df = df.withColumn('next_stop_id_clean', gate_function('next_stop_id_s')) \n",
    "\n",
    "sbb = sbb.join(df, sbb.bpuic == df.stop_id_clean, \"left_semi\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the next stop name\n",
    "sbb=sbb.withColumn(\"next_stop_name\", f.lead(sbb.stop_name).over(Window.partitionBy(sbb.trip_id).orderBy(sbb.arrival_time)))\\\n",
    ".withColumn(\"next_stop_bpuic\", f.lead(sbb.bpuic).over(Window.partitionBy(sbb.trip_id).orderBy(sbb.arrival_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The arrival & departure times columns contain dates so we remove those\n",
    "# We already have a \"date\" column sbb\n",
    "sbb = sbb.withColumn('arrival_time',f.substring(sbb.arrival_time,12,15)).\\\n",
    "withColumn('actual_arrival_time',f.substring(sbb.actual_arrival_time,12,15)).\\\n",
    "withColumn('departure_time',f.substring(sbb.departure_time,12,15)).\\\n",
    "withColumn('actual_departure_time',f.substring(sbb.actual_departure_time,12,15)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add \":00\" (seconds) to the time columns because they are in HH:MM format and we want them in the HH:MM:SS fmt\n",
    "seconds_function = udf(lambda x: x + \":00\", StringType())\n",
    "sbb = sbb.withColumn('arrival_time', seconds_function('arrival_time'))\n",
    "sbb = sbb.withColumn('departure_time', seconds_function('departure_time')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove weekends\n",
    "sbb = sbb.withColumn(\"input_timestamp\",to_timestamp(col(\"date\"),\"dd.MM.yyyy\")).withColumn(\"week_day_number\", date_format(col(\"input_timestamp\"), \"u\")) #add day column\n",
    "sbb = sbb.filter(sbb.week_day_number < 6)\n",
    "sbb = sbb.drop(*['input_timestamp', 'week_day_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hours not between 6AM and 7PM\n",
    "sbb = sbb.filter((f.hour('departure_time')>6)&(f.hour('arrival_time')<19))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the delays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the scheduled times (`arrival_time` column) and the actual times (`actual_arrival_time` column). We would like to create a new column `arrival_delay` that shows the arrival delay for each trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define function to compute the delay: computes the difference between actual time and scheduled time\n",
    "def compute_delay(actual, planned):\n",
    "    #planned += ':00'\n",
    "    format_ = '%H:%M:%S'\n",
    "    actual_time = datetime.strptime(actual, format_)\n",
    "    planned_time = datetime.strptime(planned, format_)\n",
    "    \n",
    "    if (actual_time < planned_time):\n",
    "        return 0\n",
    "    \n",
    "    delay = actual_time - planned_time\n",
    "    delay = int(ceil(delay.seconds/60.0))\n",
    "    return delay\n",
    "\n",
    "delay_function = udf(lambda x: compute_delay(x[0], x[1]), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add arrival & departure delay columns to sbb\n",
    "sbb = sbb.withColumn('arrival_delay', delay_function(array('actual_arrival_time', 'arrival_time')))\n",
    "sbb = sbb.withColumn('departure_delay',delay_function(array('actual_departure_time', 'departure_time')))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbb_filtered = sbb.withColumn('arrival_hour', f.hour('arrival_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sbb_filtered.write.format(\"orc\").save(\"/user/ellouz/sbb_filtered_final.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbb_filtered = spark.read.orc(\"/user/ellouz/sbb_filtered_final.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------+--------------+----------+----------------+------------+-------------------+--------------+---------------------+-------+----------------+---------------+-------------+---------------+------------+\n",
      "|      date|        trip_id|line_id|transport_type|circuit_id|       stop_name|arrival_time|actual_arrival_time|departure_time|actual_departure_time|  bpuic|  next_stop_name|next_stop_bpuic|arrival_delay|departure_delay|arrival_hour|\n",
      "+----------+---------------+-------+--------------+----------+----------------+------------+-------------------+--------------+---------------------+-------+----------------+---------------+-------------+---------------+------------+\n",
      "|14.05.2019|85:11:14047:002|  14047|           Zug|          |      Effretikon|    12:22:00|           12:23:07|      12:23:00|             12:23:36|8503305|     Bassersdorf|        8503307|            2|              1|          12|\n",
      "|14.05.2019|85:11:14047:002|  14047|           Zug|          |     Bassersdorf|    12:27:00|           12:26:54|      12:27:00|             12:27:39|8503307|Zürich Flughafen|        8503016|            0|              1|          12|\n",
      "|14.05.2019|85:11:14047:002|  14047|           Zug|          |Zürich Flughafen|    12:32:00|           12:31:00|      12:34:00|             12:34:36|8503016| Zürich Oerlikon|        8503006|            0|              1|          12|\n",
      "|14.05.2019|85:11:14047:002|  14047|           Zug|          | Zürich Oerlikon|    12:38:00|           12:38:20|      12:39:00|             12:39:53|8503006|Zürich Wipkingen|        8503015|            1|              1|          12|\n",
      "|14.05.2019|85:11:14047:002|  14047|           Zug|          |Zürich Wipkingen|    12:41:00|           12:42:03|      12:41:00|             12:42:52|8503015|       Zürich HB|        8503000|            2|              2|          12|\n",
      "+----------+---------------+-------+--------------+----------+----------------+------------+-------------------+--------------+---------------------+-------+----------------+---------------+-------------+---------------+------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "sbb_filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We add the arrival_hour_df column in case we'd like to use the second approach\n",
    "df = df.withColumn('arrival_hour_df', f.hour('arrival_time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|   route_desc| count|\n",
      "+-------------+------+\n",
      "|          TGV|    11|\n",
      "|     Eurocity|    50|\n",
      "|Standseilbahn|  1815|\n",
      "| RegioExpress|   166|\n",
      "|       S-Bahn| 18073|\n",
      "| Luftseilbahn|   176|\n",
      "|         Taxi|   133|\n",
      "|         Tram| 63415|\n",
      "|          ICE|    18|\n",
      "|          Bus|195044|\n",
      "|       Schiff|  1079|\n",
      "|   InterRegio|   794|\n",
      "|    Intercity|   518|\n",
      "+-------------+------+"
     ]
    }
   ],
   "source": [
    "df.groupBy(df.route_desc).count().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df cleaning: replace transport type with category name \n",
    "df = df.replace(['TGV', 'Eurocity', 'RegioExpress', 'S-Bahn', 'ICE', 'InterRegio', 'Intercity'], 'Zug', 'route_desc')\\\n",
    ".replace(['Standseilbahn', 'Luftseilbahn', 'Taxi', 'Schiff'], '', 'route_desc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.write.format(\"orc\").save(\"/user/ellouz/df_for_merge.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.orc(\"/user/ellouz/df_for_merge.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a distribution to the delays: two approaches\n",
    "[Several studies](https://ascelibrary.org/doi/pdf/10.1061/%28ASCE%29TE.1943-5436.0000724) recommend the **lognormal** distribution as the descriptor of day-to-day public transport travel time. Since our data involves trains, buses, and other transport types, we find this distribution fitting to our study and decide to model the travel times using it.\n",
    "\n",
    "To that end, we constitute groups of delays. \n",
    "1) A **first approach** would be to create a group of delays for each transport type present in the SBB data. After that, we can fit the distribution on each group and get a set of parameters for each transport type. As we see below, there are 4 categories: Trams, Trains (Zug), Buses, and other types of transport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+\n",
      "|transport_type|   count|\n",
      "+--------------+--------+\n",
      "|          Tram|30950230|\n",
      "|           Zug| 5122675|\n",
      "|           Bus|  320042|\n",
      "|              |    2036|\n",
      "+--------------+--------+"
     ]
    }
   ],
   "source": [
    "sbb_filtered.groupBy('transport_type').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) A **second approach** is to isolate the delays of each ( `stop_id, arrival_time, transport_type`) group. This means that we model each station at each arrival time and for each transport type. The benefit of this approach is that the delays become more specific to the station and to the time, especially if we transform `arrival_time` into `arrival_hour` to make bigger groups. Just like approach 1, we would get a set of parameters for each group.\n",
    "\n",
    "Each approach has its pros and cons and in the following, both implementations are present. However, due to time constraints and because of the few bugs the second approach presented, we decided to adopt approach 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define a `compute_params` function that is common to both approaches. It takes a list of delays and use `stats.lognorm.fit` to retrieve the corresponding parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#compute the lognorm parameter, return None if list is empty\n",
    "#returns a list of 3 floats\n",
    "def compute_params(l): \n",
    "    if len(l) > 0: \n",
    "        params = stats.lognorm.fit(l, loc = 0)\n",
    "        return [float(x) for x in params] # need to convert to float because udf doesn't like numpy.float\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First approach: unique parameters for each transport type\n",
    "We can discard outliers by filtering out arrival delay > 100 minutes. Our reasoning was that when the delay was high, the user may have had the possibility to take another connection whose arrival time was smaller than the arrival time + arrival delay of the connection she was waiting on. Plus a high delay is unusual and usually out of control of the transport company (e.g, road closed because of an accident, storm destroying part of the railways,...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Filter out when delay > 100 (outliers)\n",
    "sbb_train = sbb_filtered.select(sbb_filtered.transport_type,sbb_filtered.arrival_delay).filter(sbb_filtered.transport_type == 'Zug').cache()\n",
    "sbb_train = sbb_train.filter(sbb_train.arrival_delay < 100).cache()\n",
    "group_arr_train = sbb_train.agg(collect_list('arrival_delay').alias('arr_delay_list'))\n",
    "compute_params_function = udf(lambda x: compute_params(x),ArrayType(FloatType())) #x is the list of the delay of a group\n",
    "group_arr_train = group_arr_train.withColumn('arr_params', compute_params_function('arr_delay_list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|arr_params                        |\n",
      "+----------------------------------+\n",
      "|[0.47517332, -0.9588776, 2.308198]|\n",
      "+----------------------------------+"
     ]
    }
   ],
   "source": [
    "group_arr_train.select(group_arr_train.arr_params).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tram delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sbb_trams_sample = sbb_trams.sample(False, 0.01, seed=10)\n",
    "# sbb_trams_sample.write.format(\"orc\").save(\"/user/ellouz/trams_sample.orc\")\n",
    "sbb_trams_sample = spark.read.orc(\"/user/ellouz/trams_sample.orc\")\n",
    "sbb_trams_sample = sbb_filtered.select(sbb_filtered.transport_type,sbb_filtered.arrival_delay).filter(sbb_filtered.transport_type == 'Tram')\n",
    "sbb_trams_sample = sbb_trams_sample.filter(sbb_trams_sample.arrival_delay < 100)\n",
    "group_arr_tram = sbb_trams_sample.agg(collect_list('arrival_delay').alias('arr_delay_list'))\n",
    "compute_params_function = udf(lambda x: compute_params(x),ArrayType(FloatType())) #x is the list of the delay of a group\n",
    "group_arr_tram = group_arr_tram.withColumn('arr_params', compute_params_function('arr_delay_list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_arr_tram.select('arr_params').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bus delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbb_buses = sbb_filtered.select(sbb_filtered.transport_type,sbb_filtered.arrival_delay).filter(sbb_filtered.transport_type == 'Bus').cache()\n",
    "sbb_buses = sbb_buses.filter(sbb_buses.arrival_delay < 100).cache()\n",
    "group_arr_bus = sbb_buses.agg(collect_list('arrival_delay').alias('arr_delay_list'))\n",
    "compute_params_function = udf(lambda x: compute_params(x),ArrayType(FloatType())) #x is the list of the delay of a group\n",
    "group_arr_bus = group_arr_bus.withColumn('arr_params', compute_params_function('arr_delay_list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|arr_params                          |\n",
      "+------------------------------------+\n",
      "|[0.56315994, -4.974291E-4, 1.668936]|\n",
      "+------------------------------------+"
     ]
    }
   ],
   "source": [
    "group_arr_bus.select('arr_params').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Other transport types delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sbb_others = sbb_filtered.select(sbb_filtered.transport_type,sbb_filtered.arrival_delay).filter(sbb_filtered.transport_type == '').cache()\n",
    "sbb_others = sbb_others.filter(sbb_others.arrival_delay < 100).cache()\n",
    "group_arr_others = sbb_others.agg(collect_list('arrival_delay').alias('arr_delay_list'))\n",
    "compute_params_function = udf(lambda x: compute_params(x),ArrayType(FloatType())) #x is the list of the delay of a group\n",
    "group_arr_others = group_arr_others.withColumn('arr_params', compute_params_function('arr_delay_list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|arr_params                          |\n",
      "+------------------------------------+\n",
      "|[0.48570195, -0.91664267, 2.5085754]|\n",
      "+------------------------------------+"
     ]
    }
   ],
   "source": [
    "group_arr_others.select('arr_params').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_dict = {'Zug': [0.47517332, -0.9588776, 2.308198], 'Bus': [0.56315994, -4.974291E-4, 1.668936], \n",
    "                  'Tram': [4.607556, 1.0, 5.1064895E-9], '':[0.48570195, -0.91664267, 2.5085754]}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second approach: unique parameters for each (station_id, arrival_hour, transport_type) group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Computing the parameters of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a group for each ('stop_id', 'arrival_hour', 'transport_type')   tuple\n",
    "grouped_delays_hour = sbb_filtered.groupBy('bpuic', 'arrival_hour', 'transport_type')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/40006395/applying-udfs-on-groupeddata-in-pyspark-with-functioning-python-example#40030740\n",
    "group_arr_hour = grouped_delays_hour.agg(collect_list('arrival_delay').alias('arr_delay_list'))\n",
    "\n",
    "compute_params_function = udf(lambda x: compute_params(x),ArrayType(FloatType())) #x is the list of the delay of a group\n",
    "group_arr_hour = group_arr_hour.withColumn('arr_params', compute_params_function('arr_delay_list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_arr_hour.write.format(\"orc\").save(\"/user/ellouz/group_arr_hour.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "group_arr_hour = spark.read.orc(\"/user/ellouz/group_arr_hour.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------------+--------------------+--------------------+\n",
      "|  bpuic|arrival_hour|transport_type|      arr_delay_list|          arr_params|\n",
      "+-------+------------+--------------+--------------------+--------------------+\n",
      "|8503010|          11|           Zug|[0, 0, 0, 0, 1, 0...|[7.2196875, 1.0, ...|\n",
      "|8503016|          14|           Zug|[4, 1, 4, 1, 1, 1...|[9.560064, 1.0, 2...|\n",
      "|8503053|           8|           Zug|[3, 5, 1, 3, 3, 2...|[12.998721, 1.0, ...|\n",
      "|8503307|          15|           Zug|[2, 0, 1, 1, 2, 1...|[9.904795, 1.0, 1...|\n",
      "|8503312|          13|           Zug|[1, 0, 1, 0, 1, 0...|[7.035858, 1.0, 0...|\n",
      "+-------+------------+--------------+--------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "group_arr_hour.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merging arrival delays parameters with df (general trips data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned the sbb and df data, we merge sbb with df. We keep all entries of df since they contains all general trips. Sometimes a trip from df is not found in sbb: in this case, our strategy is to use the general expected delay for the corresponding means of transport computed in approach 1. For example, if there is a certain bus trip which does not have any corresponding trips in sbb, we use the parameters of the lognormal distribution for all bus delays in sbb and predict a delay for that bus trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_hour = [group_arr_hour.bpuic == df.stop_id_clean, group_arr_hour.arrival_hour == df.arrival_hour_df, group_arr_hour.transport_type == df.route_desc]\n",
    "left_joined_hour = df.join(group_arr_hour, cond_hour, \"left\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left_joined_hour.write.format(\"orc\").save(\"/user/ellouz/left_joined_hour.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "left_joined_hour = spark.read.orc(\"/user/ellouz/left_joined_hour.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+--------------------+-------+------------+--------------+-------------+-----------+-------------+-------------------+----------------+----------------+-------------+--------------+--------------+---------------+---------------+------------+------+-------+---------+--------+------+--------+------+----------+--------+---------+----------------+---------------+----------+----------+---------+--------------+-----------------+-------------------+----------------+----------------+-------------------+--------------------+-----------+-------------+------------------+---------------+-------+------------+--------------+--------------------+--------------------+\n",
      "|     route_id|service_id|             trip_id|stop_id|arrival_time|departure_time|stop_sequence|pickup_type|drop_off_type|          stop_name|        stop_lat|        stop_lon|location_type|parent_station|Dist_to_zurich|  trip_headsign|trip_short_name|direction_id|monday|tuesday|wednesday|thursday|friday|saturday|sunday|start_date|end_date|agency_id|route_short_name|route_long_name|route_desc|route_type|stop_id_s|next_stop_id_s|next_arrival_time|next_departure_time|   next_stop_lat|   next_stop_lon|     next_stop_name|        next_trip_id|Travel_time|stop_id_clean|next_stop_id_clean|arrival_hour_df|  bpuic|arrival_hour|transport_type|      arr_delay_list|          arr_params|\n",
      "+-------------+----------+--------------------+-------+------------+--------------+-------------+-----------+-------------+-------------------+----------------+----------------+-------------+--------------+--------------+---------------+---------------+------------+------+-------+---------+--------+------+--------+------+----------+--------+---------+----------------+---------------+----------+----------+---------+--------------+-----------------+-------------------+----------------+----------------+-------------------+--------------------+-----------+-------------+------------------+---------------+-------+------------+--------------+--------------------+--------------------+\n",
      "|26-14-A-j19-1|  TA+b0003|1009.TA.26-14-A-j...|8591381|    06:58:00|      07:00:00|            9|          0|            0|Zürich, Stauffacher|47.3734251680677|8.52925209562258|         null|          null|    0.98066115|Zürich, Seebach|           3954|           1|     1|      1|        1|       1|     1|       0|     0|  20181209|20191214|     3849|              14|           null|      Tram|       900|  8591381|       8591367|         07:02:00|           07:02:00|47.3760957774096|8.53420181283877|Zürich, Sihlpost/HB|1009.TA.26-14-A-j...|        4.0|      8591381|           8591367|              6|8591381|           6|          Tram|[3, 1, 3, 3, 1, 3...|[5.856377, 1.0, 0...|\n",
      "+-------------+----------+--------------------+-------+------------+--------------+-------------+-----------+-------------+-------------------+----------------+----------------+-------------+--------------+--------------+---------------+---------------+------------+------+-------+---------+--------+------+--------+------+----------+--------+---------+----------------+---------------+----------+----------+---------+--------------+-----------------+-------------------+----------------+----------------+-------------------+--------------------+-----------+-------------+------------------+---------------+-------+------------+--------------+--------------------+--------------------+\n",
      "only showing top 1 row"
     ]
    }
   ],
   "source": [
    "left_joined_hour.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting delay based on the probability given by the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed the **parameters of the lognorm distribution** for the arrival delay of each entry of our dataframe, we decided to **predict a delay based on a probability**. \n",
    "For example if a user wants 80% certainty of her route, we assumed that it will need each leg of the final itinerary to have a certainty probability of 80% (similar to the weakest link in computer network). So to compute a delay with 80% certainty, we compute the **percent point function of the lognorm** that **given a probability, returns the maximum delay such that 80% of the possible delay are less or equal to this maximum delay**. With this reasoning it means that if we want 100% certainty, it will give back a delay equals to infinity. So the bigger the probability given by the user, the bigger the delay computed. \n",
    "To simplify our model, we precomputed the delay for 4 different probabilities from which the user can choose : \n",
    "- 80%\n",
    "- 85%\n",
    "- 90%\n",
    "- 95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a `predict_delay` function that computes the delay given a percentile and a list of parameters for the distribution. \n",
    "For the second approach, the `predict_delay_2nd_approach` version takes into account the cases where trips are not found in the sbb data and uses the parameters of the transport type in those cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_delay(transport_type, q): \n",
    "    \n",
    "    type_parameters = transport_dict[transport_type]\n",
    "    shape, loc, scale = type_parameters[0], type_parameters[1], type_parameters[2]\n",
    "        \n",
    "    return int(ceil(stats.lognorm.ppf(q, shape, loc, scale)))\n",
    "\n",
    "\n",
    "def predict_delay_2nd_approach(transport_type, parameters, q): \n",
    "#     If the parameters are already computed\n",
    "    if parameters != None:\n",
    "        shape, loc, scale = parameters[0], parameters[1], parameters[2]\n",
    "#     Else use the Bus/Zug/Tram parameters\n",
    "    else:\n",
    "        type_parameters = transport_dict[transport_type]\n",
    "        shape, loc, scale = type_parameters[0], type_parameters[1], type_parameters[2]\n",
    "        \n",
    "    return int(ceil(stats.lognorm.ppf(q, shape, loc, scale)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First approach: by transport type\n",
    "predict_delay_function = udf(lambda x, y: predict_delay(x, y),IntegerType())\n",
    "joined_delay = left_joined_hour.withColumn('pred_delay_80', predict_delay_function('route_desc', lit(0.80)))\\\n",
    ".withColumn('pred_delay_85', predict_delay_function('route_desc', lit(0.85)))\\\n",
    ".withColumn('pred_delay_90', predict_delay_function('route_desc', lit(0.90)))\\\n",
    ".withColumn('pred_delay_95', predict_delay_function('route_desc', lit(0.95)))\n",
    "\n",
    "# Second approach: by (station, hour, transport_type)\n",
    "# predict_delay_function = udf(lambda x, y, z: predict_delay(x, y, z),IntegerType())\n",
    "# joined_delay = left_joined_hour.withColumn('pred_delay_80', predict_delay_function('route_desc', 'arr_params', lit(0.80)))\\\n",
    "# .withColumn('pred_delay_85', predict_delay_function('route_desc', 'arr_params', lit(0.85)))\\\n",
    "# .withColumn('pred_delay_90', predict_delay_function('route_desc', 'arr_params', lit(0.90)))\\\n",
    "# .withColumn('pred_delay_95', predict_delay_function('route_desc', 'arr_params', lit(0.95)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove unneeded columns\n",
    "columns_to_drop = ['drop_off_type', 'stop_lat', 'stop_lon', 'Dist_to_zurich', 'trip_headsign', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday','saturday', 'sunday']\n",
    "joined_delay = joined_delay.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While observing our results from the second approach, we found out that 8 entries of the pred_delay_95 columns had null values because the delay computed was too big. Therefore we replaced the null values with a delay of 100 minutes instead. We also found out that some delays computed were negative or way too big therefore we also considered them equal to 100 minutes in the function ```compute_new_arrival```. Our reasoning is the same than when we computed the general lognorm distribution for each transport type. We think it is reasonable to assume that a delay > 100 minutes is not SBB's fault and unusual, and really bad luck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Second approach related. This is not used in the first approach\n",
    "# joined_delay = joined_delay.fillna({'pred_delay_95' : str(100)}) #we found out that 8 entries of this column were null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined_delay.write.format(\"orc\").save(\"/user/ellouz/joined_delay_final.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "joined_delay = spark.read.orc(\"/user/ellouz/joined_delay_final.orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have different delays that corresponds to different certainty degrees (or success probabilities), we can **add this delay to the arrival time** of a trip and then calculate the shortest path using this new arrival time as input to our graph. So the **time-dependent part of the shortest path was modeled in the parameters we gave to the algorithm**. Instead of answering to the question \"I will have a delay of x with this train, so will I make it to that train that departs at hh:mm?\", we simplified it by saying \"My train will have a delay of x minutes, so I will arrive at hh:mm + x minutes instead of hh:mm which I now consider as the new arrival time of my train\".\n",
    "\n",
    "*Example*:\n",
    "\n",
    "Let's say there is a train A that was supposed to arrive at 10:30. However, based on the previous delays we were able to compute, we fitted a lognorm distribution on it to get the lognorm parameters. Then we computed the maximal delay of the distribution such that 80% of the delay distribution is smaller or equal than this value, it gave us 3 minutes of delay. Now we did the same thing for 90% which gave us 4 minutes of delay.\n",
    "Now we changed the arrival time to 10:33 and to 10:34 respectively. When the user selects the certainty she wants (80 or 90% in this example), we will change the arrival time of train A to the one corresponding to the certainty level. We do that for our entire dataset before constructing the graph that we will then feed to the shortest path algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the delay to the arrival time\n",
    "# delay is a str(delay_in_minutes)\n",
    "def compute_new_arrival(delay, arrival_time):\n",
    "    if((int(delay) < 0) | (int(delay) > 100)):\n",
    "        delay = 100\n",
    "    delay_dt = timedelta(minutes=int(delay))\n",
    "    format_ = '%H:%M:%S'\n",
    "    actual_time = datetime.strptime(arrival_time, format_)\n",
    "    new_time = actual_time + delay_dt\n",
    "    return str(new_time.time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_new_arrival_function = udf(lambda x: compute_new_arrival(x[0], x[1]), StringType())\n",
    "joined_delay = joined_delay.withColumn('arrival_time_80', compute_new_arrival_function(array('pred_delay_80','arrival_time')))\\\n",
    ".withColumn('arrival_time_85', compute_new_arrival_function(array('pred_delay_85','arrival_time')))\\\n",
    ".withColumn('arrival_time_90', compute_new_arrival_function(array('pred_delay_90','arrival_time')))\\\n",
    ".withColumn('arrival_time_95', compute_new_arrival_function(array('pred_delay_95','arrival_time')))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of first approach (delays by transport type)\n",
    "#joined_delay.write.format(\"orc\").save(\"/user/ellouz/delayed_arrival_times_basic.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of second approach (by station, hour, trnasport_type)\n",
    "#joined_delay.write.format(\"orc\").save(\"/user/ellouz/df_final9.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1st approach\n",
    "df_basic = spark.read.orc(\"/user/ellouz/delayed_arrival_times_basic.orc\")\n",
    "important_columns = ['stop_id_s', 'trip_id', 'arrival_time', 'departure_time', 'next_stop_id_s', 'next_trip_id',\\\n",
    "                'next_arrival_time', 'next_departure_time', 'Travel_time', 'stop_name', 'arrival_time_80',\\\n",
    "               'arrival_time_85', 'arrival_time_90', 'arrival_time_95', 'arrival_time_99']\n",
    "edges=df_basic.select(important_columns).fillna({'Travel_time':0})\n",
    "#edges.write.parquet(\"/user/eckes/edges.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd approach\n",
    "# df_final = spark.read.orc(\"/user/ellouz/df_final9.orc\")\n",
    "# important_columns = ['stop_id_s', 'trip_id', 'arrival_time', 'departure_time', 'next_stop_id_s', 'next_trip_id',\\\n",
    "#                 'next_arrival_time', 'next_departure_time', 'Travel_time', 'stop_name', 'arrival_time_80',\\\n",
    "#                'arrival_time_85', 'arrival_time_90', 'arrival_time_95']\n",
    "# edges=df_final.select(important_columns).fillna({'Travel_time':0})\n",
    "# edges.write.parquet(\"/user/abourjei/edges_2.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
